{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece6190d",
   "metadata": {},
   "source": [
    "**第一部分 导入库和设置基本参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2c1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f7d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "min_size, max_size = 600, 1000\n",
    "\n",
    "RPN_config = {'anchor_scale' : (128, 256, 512), 'anchor_aspect_ratio' : (0.5, 1.0, 2.0), 'downsample' : 16, \n",
    "              'in_channels' : 512, 'num_anchors' : 9,\n",
    "              'bbox_reg_weights' : (1., 1., 1., 1.),\n",
    "              'iou_positive_thresh' : 0.7, 'iou_negative_high' : 0.3, 'iou_negative_low' : 0,\n",
    "              'batch_size_per_image' : 256, 'positive_fraction' : 0.5, \n",
    "              'min_size' : 16, 'nms_thresh' : 0.7, \n",
    "              'top_n_train' : 2000, 'top_n_test' : 300}\n",
    "\n",
    "FastRCNN_config = {'output_size' : 7, 'downsample' : 16, \n",
    "                   'out_channels' : 4096, 'num_classes' : 21,\n",
    "                   'bbox_reg_weights' : (10., 10., 5., 5.),\n",
    "                   'iou_positive_thresh' : 0.5, 'iou_negative_high' : 0.5, 'iou_negative_low' : 0.1,\n",
    "                   'batch_size_per_image' : 128, 'positive_fraction' : 0.25, \n",
    "                   'min_size' : 1, 'nms_thresh' : 0.3, \n",
    "                   'score_thresh' : 0.05, 'top_n' : 50}\n",
    "\n",
    "TRAIN_config = {'epochs' : 15,\n",
    "                'lr' : 0.001, 'momentum' : 0.9, 'weight_decay' : 0.0005,\n",
    "                'milestones' : [10], 'clip' : 10,\n",
    "                'epoch_freq' : 1, 'print_freq' : 1,\n",
    "                'save' : True, 'SAVE_PATH' : './'}\n",
    "\n",
    "TEST_config = {'num_classes' : 21, 'iou_thresh' : 0.5, 'use_07_metric' : True}\n",
    "\n",
    "DEMO_config = {'min_size' : min_size, 'mean' : imagenet_mean, 'std' : imagenet_std, 'score_thresh' : 0.7}\n",
    "\n",
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc503b",
   "metadata": {},
   "source": [
    "**第二部分 数据获取及处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b76051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as Func\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, bboxs):\n",
    "        for t in self.transforms:\n",
    "            image, bboxs = t(image, bboxs)\n",
    "        return image, bboxs\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    Only applied to image, not bboxes.\n",
    "    \"\"\"\n",
    "    def __call__(self, image, bboxs):\n",
    "        return Func.to_tensor(image), bboxs\n",
    "    \n",
    "    \n",
    "class Normalize(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize a tensor image with mean and standard deviation.\n",
    "    Only applied to image, not bboxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, image, bboxs):\n",
    "        return Func.normalize(image, self.mean, self.std, self.inplace), bboxs\n",
    "    \n",
    "    \n",
    "class Resize(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Resize the short side of image to given size.\n",
    "    Assume the coords are given min_x, min_y, max_x, max_y.\n",
    "    Both applied to image and bboxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_size, max_size):\n",
    "        super().__init__()\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def forward(self, image, bboxs):\n",
    "        return resize(image, bboxs, self.min_size, self.max_size)\n",
    "    \n",
    "    \n",
    "class Flip(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Apply horizontal flip on image and bboxes.\n",
    "    Assume the coords are given min_x, min_y, max_x, max_y.\n",
    "    Both applied to image and bboxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, image, bboxs):\n",
    "        if torch.rand(1) < self.p:\n",
    "            flip_image = ImageOps.mirror(image)\n",
    "            if bboxs == None:\n",
    "                return flip_image, bboxs\n",
    "            else:\n",
    "                flip_bbox = flip(image, bboxs)\n",
    "                return flip_image, flip_bbox\n",
    "        else:\n",
    "            return image, bboxs\n",
    "            \n",
    "            \n",
    "def resize(img, bboxs, min_size, max_size):\n",
    "    w, h = img.size\n",
    "    min_side, max_side = min(w, h), max(w, h)\n",
    "    \n",
    "    ratio = min(min_size / min_side, max_size / max_side)\n",
    "    resize_w, resize_h = int(ratio * w), int(ratio * h)\n",
    "    ratio_w, ratio_h = resize_w / w, resize_h / h\n",
    "    \n",
    "    resize_img = img.resize((resize_w, resize_h), resample=Image.BILINEAR)\n",
    "    resize_bboxs = bboxs.clone()\n",
    "    if bboxs != None:\n",
    "        resize_bboxs[:, 0::2] = bboxs[:, 0::2] * ratio_w\n",
    "        resize_bboxs[:, 1::2] = bboxs[:, 1::2] * ratio_h\n",
    "    return resize_img, resize_bboxs\n",
    "            \n",
    "            \n",
    "def flip(img, bboxs):\n",
    "    img = Func.pil_to_tensor(img)\n",
    "    _, h, w = img.shape\n",
    "\n",
    "    flip_bboxs = []\n",
    "    for bbox in bboxs:\n",
    "        min_x, min_y, max_x, max_y = bbox\n",
    "        flip_min_x, flip_max_x = w-min_x, w-max_x\n",
    "        flip_bboxs.append(torch.FloatTensor([flip_max_x, min_y, flip_min_x, max_y]))\n",
    "    return torch.stack(flip_bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179b7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC_Detection(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, year, image_set, download, transforms, use_diff):\n",
    "        self.dataset = torchvision.datasets.VOCDetection(root, year, image_set, download)\n",
    "        self.transforms = transforms\n",
    "        self.use_diff = use_diff\n",
    "        self.VOC_LABELS = ('__background__', # always index 0\n",
    "                           'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', \n",
    "                           'diningtable', 'dog', 'horse','motorbike', 'person', 'pottedplant', 'sheep', 'sofa', \n",
    "                           'train', 'tvmonitor')\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.dataset[idx]\n",
    "        labels, bboxs = [], []\n",
    "        for info in target['annotation']['object']:\n",
    "            if self.use_diff or (int(info['difficult']) == 0):\n",
    "                labels.append(self.VOC_LABELS.index(info['name']))\n",
    "                # Make pixel indexes 0-based\n",
    "                bboxs.append(torch.FloatTensor([float(info['bndbox']['xmin'])-1, float(info['bndbox']['ymin'])-1, \n",
    "                                                float(info['bndbox']['xmax'])-1, float(info['bndbox']['ymax'])-1]))\n",
    "        \n",
    "        labels, bboxs = torch.tensor(labels, dtype=int), torch.stack(bboxs, dim=0)\n",
    "        if self.transforms: img, bboxs = self.transforms(img, bboxs)\n",
    "        return img, labels, bboxs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dd7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "data_dir = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f130ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([Resize(min_size, max_size), \n",
    "                           Flip(), \n",
    "                           ToTensor(), \n",
    "                           Normalize(mean=imagenet_mean, std=imagenet_std)])\n",
    "test_transform = Compose([Resize(min_size, max_size), \n",
    "                          ToTensor(), \n",
    "                          Normalize(mean=imagenet_mean, std=imagenet_std)])\n",
    "\n",
    "train_dataset = VOC_Detection(root=data_dir, year='2012', image_set='train', \n",
    "                                      download=False, transforms=train_transform, use_diff=False)\n",
    "test_dataset = VOC_Detection(root=data_dir, year='2012', image_set='val', \n",
    "                                     download=False, transforms=test_transform, use_diff=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a14c7",
   "metadata": {},
   "source": [
    "**第三部分 utils工具函数定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ed4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Balanced_Sampler():\n",
    "    def __init__(self, batch_size_per_image, positive_fraction):\n",
    "        self.batch_size_per_image = batch_size_per_image\n",
    "        self.positive_fraction = positive_fraction\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        sampled_positive_masks = []\n",
    "        sampled_negative_masks = []\n",
    "        for labels_per_image in labels:\n",
    "            positive_idx = torch.where(labels_per_image >= 1)[0]\n",
    "            negative_idx = torch.where(labels_per_image == 0)[0]\n",
    "\n",
    "            num_positive = int(self.batch_size_per_image * self.positive_fraction)\n",
    "            num_positive = min(positive_idx.numel(), num_positive)\n",
    "            \n",
    "            num_negative = self.batch_size_per_image - num_positive\n",
    "            num_negative = min(negative_idx.numel(), num_negative)\n",
    "            \n",
    "            sampled_positive = torch.randperm(positive_idx.numel(), device=positive_idx.device)[:num_positive]\n",
    "            sampled_negative = torch.randperm(negative_idx.numel(), device=negative_idx.device)[:num_negative]\n",
    "\n",
    "            sampled_positive_idx = positive_idx[sampled_positive]\n",
    "            sampled_negative_idx = negative_idx[sampled_negative]\n",
    "\n",
    "            sampled_positive_mask = torch.zeros_like(labels_per_image, device=labels_per_image.device).bool()\n",
    "            sampled_negative_mask = torch.zeros_like(labels_per_image, device=labels_per_image.device).bool()\n",
    "\n",
    "            sampled_positive_mask[sampled_positive_idx] = True\n",
    "            sampled_negative_mask[sampled_negative_idx] = True\n",
    "\n",
    "            sampled_positive_masks.append(sampled_positive_mask)\n",
    "            sampled_negative_masks.append(sampled_negative_mask)\n",
    "\n",
    "        return torch.stack(sampled_positive_masks, dim=0).bool(), torch.stack(sampled_negative_masks, dim=0).bool()\n",
    "\n",
    "\n",
    "class BoxCoder():\n",
    "    def __init__(self, weights=(1., 1., 1., 1.), bbox_clip=math.log(1000. / 16)):\n",
    "        self.weights = weights\n",
    "        self.bbox_clip = bbox_clip\n",
    "        \n",
    "    def decode(self, bbox_deltas, proposals):\n",
    "        widths = proposals[:, :, 2] - proposals[:, :, 0]\n",
    "        heights = proposals[:, :, 3] - proposals[:, :, 1]\n",
    "        cx = (proposals[:, :, 0] + proposals[:, :, 2]) / 2\n",
    "        cy = (proposals[:, :, 1] + proposals[:, :, 3]) / 2\n",
    "        \n",
    "        wx, wy, ww, wh = self.weights\n",
    "        dx = bbox_deltas[:, :, 0] / wx\n",
    "        dy = bbox_deltas[:, :, 1] / wy\n",
    "        dw = bbox_deltas[:, :, 2] / ww\n",
    "        dh = bbox_deltas[:, :, 3] / wh\n",
    "\n",
    "        dw = torch.clamp(dw, max=self.bbox_clip)\n",
    "        dh = torch.clamp(dh, max=self.bbox_clip)\n",
    "\n",
    "        pred_cx = cx + dx * widths\n",
    "        pred_cy = cy + dy * heights\n",
    "        pred_w = widths * torch.exp(dw)\n",
    "        pred_h = heights * torch.exp(dh)\n",
    "\n",
    "        pred_x1 = pred_cx - pred_w / 2\n",
    "        pred_y1 = pred_cy - pred_h / 2\n",
    "        pred_x2 = pred_cx + pred_w / 2\n",
    "        pred_y2 = pred_cy + pred_h / 2\n",
    "        pred_bboxs = torch.stack((pred_x1, pred_y1, pred_x2, pred_y2), dim=2)\n",
    "        return pred_bboxs\n",
    "\n",
    "    def encode(self, matched_gt_bboxs, proposals):\n",
    "        wx, wy, ww, wh = self.weights\n",
    "        \n",
    "        proposals_x1 = proposals[:, :, 0]\n",
    "        proposals_y1 = proposals[:, :, 1]\n",
    "        proposals_x2 = proposals[:, :, 2]\n",
    "        proposals_y2 = proposals[:, :, 3]\n",
    "        \n",
    "        matched_gt_bboxs_x1 = matched_gt_bboxs[:, :, 0]\n",
    "        matched_gt_bboxs_y1 = matched_gt_bboxs[:, :, 1]\n",
    "        matched_gt_bboxs_x2 = matched_gt_bboxs[:, :, 2]\n",
    "        matched_gt_bboxs_y2 = matched_gt_bboxs[:, :, 3]\n",
    "        \n",
    "        proposals_widths = proposals_x2 - proposals_x1\n",
    "        proposals_heights = proposals_y2 - proposals_y1\n",
    "        proposals_cx = (proposals_x1 + proposals_x2) / 2\n",
    "        proposals_cy = (proposals_y1 + proposals_y2) / 2\n",
    "\n",
    "        matched_gt_bboxs_widths = matched_gt_bboxs_x2 - matched_gt_bboxs_x1\n",
    "        matched_gt_bboxs_heights = matched_gt_bboxs_y2 - matched_gt_bboxs_y1\n",
    "        matched_gt_bboxs_cx = (matched_gt_bboxs_x1 + matched_gt_bboxs_x2) / 2\n",
    "        matched_gt_bboxs_cy = (matched_gt_bboxs_y1 + matched_gt_bboxs_y2) / 2\n",
    "\n",
    "        targets_dx = wx * (matched_gt_bboxs_cx - proposals_cx) / proposals_widths\n",
    "        targets_dy = wy * (matched_gt_bboxs_cy - proposals_cy) / proposals_heights\n",
    "        targets_dw = ww * torch.log(matched_gt_bboxs_widths / proposals_widths)\n",
    "        targets_dh = wh * torch.log(matched_gt_bboxs_heights / proposals_heights)\n",
    "\n",
    "        targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh), dim=2)\n",
    "        return targets\n",
    "\n",
    "\n",
    "class Matcher(object):\n",
    "    def __init__(self, iou_positive_thresh, iou_negative_high, iou_negative_low, low_quality_match):\n",
    "        self.BELOW_LOW_THRESHOLD = -1\n",
    "        self.BETWEEN_THRESHOLDS = -2\n",
    "\n",
    "        self.iou_positive_thresh = iou_positive_thresh\n",
    "        self.iou_negative_high = iou_negative_high\n",
    "        self.iou_negative_low = iou_negative_low\n",
    "        self.low_quality_match = low_quality_match\n",
    "\n",
    "    def __call__(self, match_quality_matrix):\n",
    "        proposals_max_iou_val, proposals_max_iou_idx = match_quality_matrix.max(dim=0)\n",
    "        proposals_match = proposals_max_iou_idx.clone()\n",
    "        \n",
    "        # Negative\n",
    "        negative_mask = (self.iou_negative_low <= proposals_max_iou_val) & (proposals_max_iou_val < self.iou_negative_high)\n",
    "        \n",
    "        # Not negative nor positive\n",
    "        between_mask = (self.iou_negative_high <= proposals_max_iou_val) & (proposals_max_iou_val < self.iou_positive_thresh)\n",
    "        between_mask = between_mask | (proposals_max_iou_val < self.iou_negative_low)\n",
    "        \n",
    "        proposals_max_iou_idx[negative_mask] = self.BELOW_LOW_THRESHOLD\n",
    "        proposals_max_iou_idx[between_mask] = self.BETWEEN_THRESHOLDS\n",
    "        \n",
    "        if self.low_quality_match:\n",
    "            gt_max_iou_val, _ = match_quality_matrix.max(dim=1)\n",
    "            positive_idx = torch.where(match_quality_matrix == gt_max_iou_val[:, None])[1]\n",
    "            proposals_max_iou_idx[positive_idx] = proposals_match[positive_idx]\n",
    "        return proposals_max_iou_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e462a",
   "metadata": {},
   "source": [
    "**第四部分 Region Proposal网络模型定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab7b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class AnchorGenerator(nn.Module):\n",
    "    def __init__(self, anchor_scale=(128, 256, 512), anchor_aspect_ratio=(0.5, 1.0, 2.0), downsample=16, gpu_id=0):\n",
    "        super(AnchorGenerator, self).__init__()\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        self.gpu = gpu_id\n",
    "        \n",
    "        self.anchor_scale = anchor_scale\n",
    "        self.anchor_aspect_ratio = anchor_aspect_ratio\n",
    "        self.downsample = downsample\n",
    "        self.base_anchors = self.generate_base_anchors(anchor_scale, anchor_aspect_ratio).cuda(self.gpu)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        images_anchors = self.generate_images_anchors(features, self.downsample)\n",
    "        return images_anchors\n",
    "\n",
    "    def generate_base_anchors(self, anchor_scale, anchor_aspect_ratio):\n",
    "        anchor_scale, anchor_aspect_ratio = torch.FloatTensor(anchor_scale), torch.FloatTensor(anchor_aspect_ratio)\n",
    "        anchor_h_ratio = torch.sqrt(anchor_aspect_ratio)\n",
    "        anchor_w_ratio = 1 / anchor_h_ratio\n",
    "        \n",
    "        anchor_ws = (anchor_w_ratio[:, None] * anchor_scale[None, :]).view(-1)\n",
    "        anchor_hs = (anchor_h_ratio[:, None] * anchor_scale[None, :]).view(-1)\n",
    "        \n",
    "        base_anchors = torch.stack([-anchor_ws, -anchor_hs, anchor_ws, anchor_hs], dim=1) / 2\n",
    "        return base_anchors\n",
    "\n",
    "    def generate_images_anchors(self, features, downsample):\n",
    "        features_size = [feature.shape[-2:] for feature in features]\n",
    "        \n",
    "        images_anchors = []\n",
    "        for f_h, f_w in features_size:\n",
    "            grid_y, grid_x = torch.meshgrid(torch.arange(0, f_h).cuda(self.gpu) * downsample, \n",
    "                                            torch.arange(0, f_w).cuda(self.gpu) * downsample)\n",
    "            grid_y, grid_x = grid_y.reshape(-1), grid_x.reshape(-1)\n",
    "            grid_xy = torch.stack((grid_x, grid_y, grid_x, grid_y), dim=1)\n",
    "            image_anchors = (grid_xy.view(-1, 1, 4) + self.base_anchors.view(1, -1, 4)).reshape(-1, 4)\n",
    "            images_anchors.append(image_anchors)\n",
    "            \n",
    "        return torch.stack(images_anchors, dim=0)\n",
    "\n",
    "\n",
    "class RPNHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, gpu_id):\n",
    "        super(RPNHead, self).__init__()\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        self.gpu = gpu_id\n",
    "        \n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1), \n",
    "                                  nn.ReLU()).cuda(self.gpu)\n",
    "        self.classification = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1).cuda(self.gpu)\n",
    "        self.bbox_regressor = nn.Conv2d(in_channels, 4 * num_anchors, kernel_size=1, stride=1).cuda(self.gpu)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.conv(features)\n",
    "        objectness = self.classification(features)\n",
    "        pred_bbox_deltas = self.bbox_regressor(features)\n",
    "        return objectness, pred_bbox_deltas\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, anchor_generator, rpn_head,\n",
    "                 bbox_reg_weights, \n",
    "                 iou_positive_thresh, iou_negative_high, iou_negative_low,\n",
    "                 batch_size_per_image, positive_fraction,\n",
    "                 min_size, nms_thresh, \n",
    "                 top_n_train, top_n_test):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.rpn_head = rpn_head\n",
    "        \n",
    "        self.box_coder = BoxCoder(bbox_reg_weights)\n",
    "        self.proposal_matcher = Matcher(iou_positive_thresh, iou_negative_high, iou_negative_low, low_quality_match=True)\n",
    "        self.sampler = Balanced_Sampler(batch_size_per_image, positive_fraction)\n",
    "        \n",
    "        self.min_size = min_size\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.top_n_train = top_n_train\n",
    "        self.top_n_test = top_n_test\n",
    "\n",
    "    def assign_gt_to_anchors(self, anchors, gt_labels, gt_bboxs):\n",
    "        labels, matched_gt_bboxs = [], []\n",
    "        for anchors_per_img, gt_bboxs_per_img in zip(anchors, gt_bboxs):\n",
    "            match_quality_matrix = torchvision.ops.box_iou(gt_bboxs_per_img, anchors_per_img)\n",
    "            matched_idxs_per_img = self.proposal_matcher(match_quality_matrix)\n",
    "            \n",
    "            matched_gt_bboxs_per_img = gt_bboxs_per_img[torch.clamp(matched_idxs_per_img, min=0)]\n",
    "            labels_per_img = (matched_idxs_per_img >= 0).float()\n",
    "\n",
    "            # Negative\n",
    "            negative_idxs = matched_idxs_per_img == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
    "            labels_per_img[negative_idxs] = 0.0\n",
    "\n",
    "            # Between\n",
    "            between_idxs = matched_idxs_per_img == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
    "            labels_per_img[between_idxs] = -1.0\n",
    "\n",
    "            labels.append(labels_per_img)\n",
    "            matched_gt_bboxs.append(matched_gt_bboxs_per_img)\n",
    "        \n",
    "        labels, matched_gt_bboxs = torch.stack(labels, dim=0), torch.stack(matched_gt_bboxs, dim=0)\n",
    "        return labels, matched_gt_bboxs\n",
    "    \n",
    "    def calculate_loss(self, objectness, pred_bbox_deltas, labels, regression_targets):\n",
    "        sampled_positive_masks, sampled_negative_masks = self.sampler(labels)\n",
    "        sampled_masks = sampled_positive_masks | sampled_negative_masks\n",
    "\n",
    "        sampled_objectness, sampled_labels = objectness[sampled_masks], labels[sampled_masks]\n",
    "        sampled_deltas, sampled_regression_targets = (pred_bbox_deltas[sampled_positive_masks], \n",
    "                                                      regression_targets[sampled_positive_masks])\n",
    "\n",
    "        rpn_cls_loss = F.binary_cross_entropy_with_logits(sampled_objectness, sampled_labels)\n",
    "        rpn_loc_loss = F.smooth_l1_loss(sampled_deltas, sampled_regression_targets, beta=1/9)\n",
    "        \n",
    "        return rpn_cls_loss, rpn_loc_loss\n",
    "    \n",
    "    def convert(self, bbox_cls, bbox_regression):\n",
    "        N, Ax4, H, W = bbox_regression.shape\n",
    "        A = Ax4 // 4\n",
    "        \n",
    "        bbox_cls, bbox_regression = bbox_cls.view(N, A, 1, H, W), bbox_regression.view(N, A, 4, H, W)\n",
    "        bbox_cls, bbox_regression = bbox_cls.permute(0, 3, 4, 1, 2), bbox_regression.permute(0, 3, 4, 1, 2)\n",
    "        bbox_cls, bbox_regression = bbox_cls.reshape(N, -1), bbox_regression.reshape(N, -1, 4)\n",
    "        return bbox_cls, bbox_regression\n",
    "    \n",
    "    def filter_proposals(self, images, objectness, proposals):\n",
    "        objectness_prob = torch.sigmoid(objectness)\n",
    "        filtered_proposals = []\n",
    "        for img, objectness_prob_per_img, proposals_per_img in zip(images, objectness_prob, proposals):\n",
    "            # clip to image size\n",
    "            proposals_per_img = torchvision.ops.clip_boxes_to_image(proposals_per_img, tuple(img.shape[-2:]))\n",
    "\n",
    "            # remove small proposals\n",
    "            keep_idx = torchvision.ops.remove_small_boxes(proposals_per_img, self.min_size)\n",
    "            objectness_prob_per_img, proposals_per_img = objectness_prob_per_img[keep_idx], proposals_per_img[keep_idx]\n",
    "\n",
    "            # NMS\n",
    "            keep_idx = torchvision.ops.nms(proposals_per_img, objectness_prob_per_img, self.nms_thresh)\n",
    "            objectness_prob_per_img, proposals_per_img = objectness_prob_per_img[keep_idx], proposals_per_img[keep_idx]\n",
    "            \n",
    "            # sort by objectness and select top-n\n",
    "            top_idx = torch.argsort(objectness_prob_per_img, descending=True)[:self.top_n()]\n",
    "            proposals_per_img = proposals_per_img[top_idx]\n",
    "            \n",
    "            filtered_proposals.append(proposals_per_img)\n",
    "        return torch.stack(filtered_proposals, dim=0)\n",
    "\n",
    "    def forward(self, images, features, gt_labels=None, gt_bboxs=None):\n",
    "        anchors = self.anchor_generator(features.detach())\n",
    "        objectness, pred_bbox_deltas = self.rpn_head(features)\n",
    "        objectness, pred_bbox_deltas = self.convert(objectness, pred_bbox_deltas)\n",
    "        \n",
    "        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "        filtered_proposals = self.filter_proposals(images, objectness.detach(), proposals)\n",
    "        \n",
    "        rpn_cls_loss, rpn_loc_loss = None, None\n",
    "        if self.training:\n",
    "            labels, matched_gt_bboxs = self.assign_gt_to_anchors(anchors, gt_labels, gt_bboxs)\n",
    "            regression_targets = self.box_coder.encode(matched_gt_bboxs, anchors)\n",
    "            rpn_cls_loss, rpn_loc_loss = self.calculate_loss(objectness, pred_bbox_deltas, labels, regression_targets)\n",
    "            \n",
    "        return filtered_proposals, rpn_cls_loss, rpn_loc_loss\n",
    "    \n",
    "    def top_n(self):\n",
    "        if self.training: return self.top_n_train\n",
    "        return self.top_n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7865e87",
   "metadata": {},
   "source": [
    "**第五部分 Fast-R-CNN和Faster-R-CNN网络模型定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448da99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification and regression for given features.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, downsample, backbone_fc, out_channels, num_classes, gpu_id):\n",
    "        super(RoIHead, self).__init__()\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        self.gpu = gpu_id\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fc = backbone_fc\n",
    "        self.classification = nn.Linear(out_channels, num_classes).cuda(self.gpu)\n",
    "        self.bbox_regressor = nn.Linear(out_channels, 4 * num_classes).cuda(self.gpu)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, images, features, proposals):\n",
    "        N, C, f_h, f_w = features.shape\n",
    "\n",
    "        proposals_list = [proposal for proposal in proposals]\n",
    "        bbox_features = torchvision.ops.roi_pool(features, proposals_list, self.output_size, 1 / self.downsample)\n",
    "        bbox_features = bbox_features.view(N, -1, C, self.output_size, self.output_size)\n",
    "        \n",
    "        bbox_features = torch.flatten(bbox_features, start_dim=2)\n",
    "        bbox_features = self.fc(bbox_features)\n",
    "        \n",
    "        objectness = self.classification(bbox_features)\n",
    "        pred_bbox_deltas = self.bbox_regressor(bbox_features)\n",
    "        return objectness, pred_bbox_deltas\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        nn.init.normal_(self.classification.weight, 0, 0.01)\n",
    "        nn.init.normal_(self.bbox_regressor.weight, 0, 0.001)\n",
    "        \n",
    "        nn.init.constant_(self.classification.bias, 0)\n",
    "        nn.init.constant_(self.bbox_regressor.bias, 0)\n",
    "\n",
    "        \n",
    "class FastRCNN(nn.Module):\n",
    "    def __init__(self, roi_head,\n",
    "                 bbox_reg_weights,\n",
    "                 iou_positive_thresh, iou_negative_high, iou_negative_low,\n",
    "                 batch_size_per_image, positive_fraction,\n",
    "                 min_size, nms_thresh, \n",
    "                 score_thresh, top_n):\n",
    "        super(FastRCNN, self).__init__()\n",
    "        self.roi_head = roi_head\n",
    "        self.num_classes = roi_head.num_classes\n",
    "        \n",
    "        self.box_coder = BoxCoder(bbox_reg_weights)\n",
    "        self.proposal_matcher = Matcher(iou_positive_thresh, iou_negative_high, iou_negative_low, low_quality_match=False)\n",
    "        self.sampler = Balanced_Sampler(batch_size_per_image, positive_fraction)\n",
    "        \n",
    "        self.min_size = min_size\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.score_thresh = score_thresh\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def assign_gt_to_proposals(self, proposals, gt_labels, gt_bboxs):\n",
    "        labels, matched_gt_bboxs = [], []\n",
    "        for proposals_per_image, gt_labels_per_image, gt_bboxs_per_image in zip(proposals, gt_labels, gt_bboxs):\n",
    "            match_quality_matrix = torchvision.ops.box_iou(gt_bboxs_per_image, proposals_per_image)\n",
    "            matched_idxs_per_image = self.proposal_matcher(match_quality_matrix)\n",
    "            \n",
    "            clamped_matched_idxs_per_image = torch.clamp(matched_idxs_per_image, min=0)\n",
    "            \n",
    "            labels_per_image = gt_labels_per_image[clamped_matched_idxs_per_image]\n",
    "            matched_gt_bboxs_per_image = gt_bboxs_per_image[clamped_matched_idxs_per_image]\n",
    "            \n",
    "            # Negative\n",
    "            negative_idxs = matched_idxs_per_image == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
    "            labels_per_image[negative_idxs] = 0.0\n",
    "\n",
    "            # Between\n",
    "            between_idxs = matched_idxs_per_image == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
    "            labels_per_image[between_idxs] = -1.0\n",
    "\n",
    "            labels.append(labels_per_image)\n",
    "            matched_gt_bboxs.append(matched_gt_bboxs_per_image)\n",
    "        return torch.stack(labels, dim=0), torch.stack(matched_gt_bboxs, dim=0)\n",
    "    \n",
    "    def calculate_loss(self, class_logits, pred_bbox_deltas, labels, regression_targets):\n",
    "        N, P, N_Cx4 = pred_bbox_deltas.shape\n",
    "        pred_bbox_deltas = pred_bbox_deltas.view(N, P, N_Cx4 // 4, 4)\n",
    "        \n",
    "        sampled_positive_masks, sampled_negative_masks = self.sampler(labels)\n",
    "        sampled_masks = sampled_positive_masks | sampled_negative_masks\n",
    "\n",
    "        sampled_class_logits, sampled_labels = class_logits[sampled_masks], labels[sampled_masks]\n",
    "        roi_cls_loss = F.cross_entropy(sampled_class_logits, sampled_labels)\n",
    "        \n",
    "        sampled_deltas, sampled_regression_targets = (pred_bbox_deltas[sampled_positive_masks], \n",
    "                                                      regression_targets[sampled_positive_masks])\n",
    "        sampled_positive_labels = labels[sampled_positive_masks]\n",
    "        sampled_regression = []\n",
    "        for sampled_positive_label, sampled_delta in zip(sampled_positive_labels, sampled_deltas):\n",
    "            sampled_regression.append(sampled_delta[sampled_positive_label])\n",
    "        \n",
    "        if len(sampled_regression) == 0:\n",
    "            roi_loc_loss = None\n",
    "        else:\n",
    "            sampled_regression = torch.stack(sampled_regression, dim=0)\n",
    "            roi_loc_loss = F.smooth_l1_loss(sampled_regression, sampled_regression_targets)\n",
    "        return roi_cls_loss, roi_loc_loss\n",
    "    \n",
    "    def convert(self, class_logits, pred_bbox_deltas, proposals):\n",
    "        # convert class logits and pred_bbox_deltas and remove background class\n",
    "        # (N, P, num_classes), (N, P, num_classes * 4) -> (N, P_without_background), (N, P_without_background, 4)\n",
    "        N, P, N_Cx4 = pred_bbox_deltas.shape\n",
    "        pred_bbox_deltas = pred_bbox_deltas.view(N, P, N_Cx4 // 4, 4)\n",
    "        probs = F.softmax(class_logits, dim=-1)\n",
    "        \n",
    "        pred_scores, pred_labels, pred_deltas, pred_proposals = [], [], [], []\n",
    "        for probs_per_img, pred_bbox_deltas_per_img, proposals_per_img in zip(probs, pred_bbox_deltas, proposals):\n",
    "            pred_scores_per_img, pred_labels_per_img = torch.max(probs_per_img[:,1:], dim=-1)\n",
    "            pred_labels_per_img += 1\n",
    "            label_map = torch.arange(self.num_classes, device=probs_per_img.device).expand_as(probs_per_img)\n",
    "            mask = label_map == pred_labels_per_img[:, None]\n",
    "            class_idx = pred_labels_per_img > 0\n",
    "            \n",
    "            pred_scores.append(pred_scores_per_img[class_idx])\n",
    "            pred_labels.append(pred_labels_per_img[class_idx])\n",
    "            pred_deltas.append(pred_bbox_deltas_per_img[mask][class_idx])\n",
    "            pred_proposals.append(proposals_per_img[class_idx])\n",
    "        \n",
    "        pred_scores, pred_labels = torch.stack(pred_scores, dim=0), torch.stack(pred_labels, dim=0)\n",
    "        pred_deltas, pred_proposals = torch.stack(pred_deltas, dim=0), torch.stack(pred_proposals, dim=0)\n",
    "        detections = self.box_coder.decode(pred_deltas, pred_proposals)\n",
    "        return pred_scores, pred_labels, detections\n",
    "    \n",
    "    def filter_detections(self, images, class_logits, pred_bbox_deltas, proposals):\n",
    "        pred_scores, pred_labels, detections = self.convert(class_logits, pred_bbox_deltas, proposals)\n",
    "        \n",
    "        filtered_scores, filtered_labels, filtered_detections = [], [], []\n",
    "        for img, scores_per_img, labels_per_img, detections_per_img in zip(images, pred_scores, pred_labels, detections):\n",
    "            # clip to image size\n",
    "            detections_per_img = torchvision.ops.clip_boxes_to_image(detections_per_img, tuple(img.shape[-2:]))\n",
    "            \n",
    "            # remove small proposals\n",
    "            keep_idx = torchvision.ops.remove_small_boxes(detections_per_img, self.min_size)\n",
    "            scores_per_img, labels_per_img, detections_per_img = (scores_per_img[keep_idx], \n",
    "                                                                  labels_per_img[keep_idx], \n",
    "                                                                  detections_per_img[keep_idx])\n",
    "            \n",
    "            # remove low score proposals\n",
    "            keep_idx = scores_per_img > self.score_thresh\n",
    "            scores_per_img, labels_per_img, detections_per_img = (scores_per_img[keep_idx], \n",
    "                                                                  labels_per_img[keep_idx], \n",
    "                                                                  detections_per_img[keep_idx])\n",
    "            \n",
    "            # NMS\n",
    "            keep_idx = torchvision.ops.batched_nms(detections_per_img, scores_per_img, labels_per_img, self.nms_thresh)\n",
    "            scores_per_img, labels_per_img, detections_per_img = (scores_per_img[keep_idx], \n",
    "                                                                  labels_per_img[keep_idx], \n",
    "                                                                  detections_per_img[keep_idx])\n",
    "            \n",
    "            # sort by scores and select top-n\n",
    "            top_idx = torch.argsort(scores_per_img, descending=True)[:self.top_n]\n",
    "            scores_per_img, labels_per_img, detections_per_img = (scores_per_img[top_idx], \n",
    "                                                                  labels_per_img[top_idx], \n",
    "                                                                  detections_per_img[top_idx])\n",
    "\n",
    "            filtered_scores.append(scores_per_img)\n",
    "            filtered_labels.append(labels_per_img)\n",
    "            filtered_detections.append(detections_per_img)\n",
    "            \n",
    "        filtered_scores = torch.stack(filtered_scores, dim=0)\n",
    "        filtered_labels = torch.stack(filtered_labels, dim=0)\n",
    "        filtered_detections = torch.stack(filtered_detections, dim=0)\n",
    "        return filtered_scores, filtered_labels, filtered_detections\n",
    "    \n",
    "    def forward(self, images, features, proposals, gt_labels=None, gt_bboxs=None):\n",
    "        class_logits, pred_bbox_deltas = self.roi_head(images, features, proposals)\n",
    "        if self.training:\n",
    "            labels, matched_gt_bboxs = self.assign_gt_to_proposals(proposals, gt_labels, gt_bboxs)\n",
    "            regression_targets = self.box_coder.encode(matched_gt_bboxs, proposals)\n",
    "            roi_cls_loss, roi_loc_loss = self.calculate_loss(class_logits, pred_bbox_deltas, labels, regression_targets)\n",
    "            return None, None, None, roi_cls_loss, roi_loc_loss\n",
    "        else:\n",
    "            pred_scores, pred_labels, pred_detections = self.filter_detections(images, \n",
    "                                                                               class_logits, pred_bbox_deltas, proposals)\n",
    "            return pred_labels, pred_scores, pred_detections, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64e19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, RPN_config, FastRCNN_config, gpu_id):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        torch.cuda.set_device(gpu_id)\n",
    "        self.gpu = gpu_id\n",
    "\n",
    "        self.backbone = self.build_backbone(gpu_id)\n",
    "        self.RPN =  self.build_RPN(RPN_config, gpu_id)\n",
    "        self.FastRCNN = self.build_FastRCNN(FastRCNN_config, gpu_id)\n",
    "        \n",
    "    def build_backbone(self, gpu_id):\n",
    "        backbone = torchvision.models.vgg16(pretrained=True).features[:30].cuda(gpu_id)\n",
    "        for i, children in enumerate(backbone.children()):\n",
    "            for child in children.parameters():\n",
    "                child.requires_grad = False\n",
    "            if i == 9: break\n",
    "        return backbone\n",
    "        \n",
    "    def build_FastRCNN(self, FastRCNN_config, gpu_id):\n",
    "        classifier = list(torchvision.models.vgg16(pretrained=True).classifier)\n",
    "        classifier = classifier[:2] + classifier[3:5]\n",
    "        backbone_fc = nn.Sequential(*classifier).cuda(self.gpu)\n",
    "        \n",
    "        roi_head = RoIHead(FastRCNN_config['output_size'], FastRCNN_config['downsample'], \n",
    "                                     backbone_fc, FastRCNN_config['out_channels'], FastRCNN_config['num_classes'], gpu_id)\n",
    "        FastRCNN_Model = FastRCNN(roi_head,\n",
    "                                      FastRCNN_config['bbox_reg_weights'],\n",
    "                                      FastRCNN_config['iou_positive_thresh'], \n",
    "                                      FastRCNN_config['iou_negative_high'], FastRCNN_config['iou_negative_low'],\n",
    "                                      FastRCNN_config['batch_size_per_image'], FastRCNN_config['positive_fraction'],\n",
    "                                      FastRCNN_config['min_size'], FastRCNN_config['nms_thresh'], \n",
    "                                      FastRCNN_config['score_thresh'], FastRCNN_config['top_n'])\n",
    "        return FastRCNN_Model\n",
    "        \n",
    "    def build_RPN(self, RPN_config, gpu_id):\n",
    "        anchor_generator = AnchorGenerator(RPN_config['anchor_scale'], RPN_config['anchor_aspect_ratio'], \n",
    "                                               RPN_config['downsample'], gpu_id)\n",
    "        rpn_head = RPNHead(RPN_config['in_channels'], RPN_config['num_anchors'], gpu_id)\n",
    "        RPN = RegionProposalNetwork(anchor_generator, rpn_head, \n",
    "                                        RPN_config['bbox_reg_weights'], \n",
    "                                        RPN_config['iou_positive_thresh'], \n",
    "                                        RPN_config['iou_negative_high'], RPN_config['iou_negative_low'],\n",
    "                                        RPN_config['batch_size_per_image'], RPN_config['positive_fraction'], \n",
    "                                        RPN_config['min_size'], RPN_config['nms_thresh'], \n",
    "                                        RPN_config['top_n_train'], RPN_config['top_n_test'])\n",
    "        return RPN\n",
    "\n",
    "    def forward(self, images, gt_labels=None, gt_bboxs=None):\n",
    "        if self.training: gt_labels, gt_bboxs = gt_labels.cuda(self.gpu), gt_bboxs.cuda(self.gpu)\n",
    "        images = images.cuda(self.gpu)\n",
    "        \n",
    "        features = self.backbone(images)\n",
    "        proposals, rpn_cls_loss, rpn_loc_loss = self.RPN(images, features, gt_labels, gt_bboxs)\n",
    "        labels, scores, detections, roi_cls_loss, roi_loc_loss = self.FastRCNN(images, features, proposals.detach(), \n",
    "                                                                               gt_labels, gt_bboxs)\n",
    "        \n",
    "        return rpn_cls_loss, rpn_loc_loss, roi_cls_loss, roi_loc_loss, labels, scores, detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f75bd2",
   "metadata": {},
   "source": [
    "**第六部分 验证函数定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bdc6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_ap(recall, precision, use_07_metric):\n",
    "    if use_07_metric:\n",
    "        ap = 0.\n",
    "        for thresh in torch.arange(0., 1.1, 0.1):\n",
    "            if torch.sum(recall >= thresh) == 0: p = 0\n",
    "            else: p = float(torch.max(precision[recall >= thresh]).cpu())\n",
    "            ap = ap + p / 11.\n",
    "    else:\n",
    "        rec = torch.cat((torch.tensor([0]), recall, torch.tensor([1])))\n",
    "        pre = torch.cat((torch.tensor([0]), precision, torch.tensor([0])))\n",
    "\n",
    "        for i in range(pre.shape[0]-1, 0, -1):\n",
    "            pre[i-1] = torch.max(pre[i-1], pre[i])\n",
    "\n",
    "        i = torch.where(rec[1:] != rec[:-1])[0]\n",
    "        ap = float(torch.sum((rec[i+1] - rec[i]) * pre[i+1]).cpu())\n",
    "    return ap\n",
    "\n",
    "\n",
    "def voc_eval(pred_bboxs, pred_labels, pred_scores, gt_bboxs, gt_labels, num_classes, iou_thresh, use_07_metric):\n",
    "    pred_bboxs_concat = torch.cat(pred_bboxs)\n",
    "    pred_labels_concat = torch.cat(pred_labels)\n",
    "    pred_scores_concat = torch.cat(pred_scores)\n",
    "    \n",
    "    gt_bboxs_concat = torch.cat(gt_bboxs).cuda(pred_bboxs_concat.device)\n",
    "    gt_labels_concat = torch.cat(gt_labels).cuda(pred_labels_concat.device)\n",
    "    \n",
    "    gt_image_ids, pred_image_ids = [], []\n",
    "    for image_id in range(len(gt_labels)): gt_image_ids += [image_id] * gt_labels[image_id].shape[0]\n",
    "    for image_id in range(len(pred_labels)): pred_image_ids += [image_id] * pred_labels[image_id].shape[0]\n",
    "    \n",
    "    gt_image_ids = torch.tensor(gt_image_ids, device=gt_labels_concat.device)\n",
    "    pred_image_ids = torch.tensor(pred_image_ids, device=pred_labels_concat.device)\n",
    "    \n",
    "    aps = []\n",
    "    for class_idx in range(1, num_classes):\n",
    "        gt_masks_per_class = gt_labels_concat == class_idx\n",
    "        gt_bboxs_per_class = gt_bboxs_concat[gt_masks_per_class]\n",
    "        gt_image_ids_per_class = gt_image_ids[gt_masks_per_class]\n",
    "        \n",
    "        check = torch.zeros_like(gt_image_ids_per_class, dtype=bool)\n",
    "        gt_num = torch.sum(gt_masks_per_class)\n",
    "        \n",
    "        pred_masks_per_class = pred_labels_concat == class_idx\n",
    "        pred_bboxs_per_class = pred_bboxs_concat[pred_masks_per_class]\n",
    "        pred_scores_per_class = pred_scores_concat[pred_masks_per_class]\n",
    "        pred_image_ids_per_class = pred_image_ids[pred_masks_per_class]\n",
    "        \n",
    "        sort_idx_per_class = torch.argsort(-pred_scores_per_class)\n",
    "        sort_pred_bboxs_per_class = pred_bboxs_per_class[sort_idx_per_class]\n",
    "        sort_pred_image_ids_per_class = pred_image_ids_per_class[sort_idx_per_class]\n",
    "        \n",
    "        pred_num = torch.sum(pred_masks_per_class)\n",
    "        tp, fp = torch.zeros(pred_num, device=pred_num.device), torch.zeros(pred_num, device=pred_num.device)\n",
    "        \n",
    "        for i in range(pred_num):\n",
    "            match_idx = torch.where(gt_image_ids_per_class == sort_pred_image_ids_per_class[i])[0]\n",
    "            if match_idx.nelement() != 0:\n",
    "                gt_bboxs_target = gt_bboxs_per_class[match_idx].clone()\n",
    "                pred_bboxs_target = sort_pred_bboxs_per_class[i].clone().view(-1, 4)\n",
    "                gt_bboxs_target[:, 2:] += 1\n",
    "                pred_bboxs_target[:, 2:] += 1\n",
    "                IoUs = torchvision.ops.box_iou(gt_bboxs_target, pred_bboxs_target).view(-1)\n",
    "                IoU_idx, IoU = torch.argmax(IoUs), torch.max(IoUs)\n",
    "                if (IoU > iou_thresh) and (check[match_idx][IoU_idx] == False):\n",
    "                    tp[i] = 1.\n",
    "                    check[match_idx][IoU_idx] = True\n",
    "                else:\n",
    "                    fp[i] = 1.\n",
    "            else:\n",
    "                fp[i] = 1.\n",
    "                \n",
    "        tp, fp = torch.cumsum(tp, dim=0), torch.cumsum(fp, dim=0)\n",
    "        recall = tp / gt_num\n",
    "        precision = tp / (tp + fp)\n",
    "        ap = voc_ap(recall, precision, use_07_metric)\n",
    "        aps.append(ap)\n",
    "    return sum(aps) / len(aps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414287d",
   "metadata": {},
   "source": [
    "**第七部分 模型训练、测试部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80bbb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN_Model():\n",
    "    def __init__(self, RPN_config, FastRCNN_config, TRAIN_config, TEST_config, DEMO_config, gpu_id):\n",
    "        self.RPN_config = RPN_config\n",
    "        self.FastRCNN_config = FastRCNN_config\n",
    "        self.TRAIN_config = TRAIN_config\n",
    "        self.TEST_config = TEST_config\n",
    "        self.DEMO_config = DEMO_config\n",
    "        self.gpu_id = gpu_id\n",
    "        \n",
    "        self.model = FasterRCNN(RPN_config, FastRCNN_config, gpu_id)\n",
    "        self.rpn_cls_losses = []\n",
    "        self.rpn_loc_losses = []\n",
    "        self.roi_cls_losses = []\n",
    "        self.roi_loc_losses = []\n",
    "        self.best_mAP = 0\n",
    "        \n",
    "    def train(self, train_loader, test_loader):\n",
    "        params = []\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key: params += [{'params': [value], \n",
    "                                              'lr': self.TRAIN_config['lr'] * 2, \n",
    "                                              'weight_decay': 0}]\n",
    "                else: params += [{'params': [value], \n",
    "                                  'lr': self.TRAIN_config['lr'], \n",
    "                                  'weight_decay': self.TRAIN_config['weight_decay']}]\n",
    "                    \n",
    "        optimizer = optim.SGD(params, momentum=self.TRAIN_config['momentum'])\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, self.TRAIN_config['milestones'])\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.TRAIN_config['epochs']):\n",
    "            print('Epoch {} Started...'.format(epoch+1))\n",
    "            for i, (images, labels, bboxs) in enumerate(train_loader):\n",
    "                rpn_cls_loss, rpn_loc_loss, roi_cls_loss, roi_loc_loss, _, _, _ = self.model(images, labels, bboxs)\n",
    "                \n",
    "                if roi_loc_loss != None: train_loss = rpn_cls_loss + rpn_loc_loss + roi_cls_loss + roi_loc_loss\n",
    "                else: train_loss = rpn_cls_loss + rpn_loc_loss + roi_cls_loss\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "                if ((i+1) % self.TRAIN_config['print_freq'] == 0) and (roi_loc_loss != None):\n",
    "                    rpn_c, rpn_l = rpn_cls_loss.item(), rpn_loc_loss.item()\n",
    "                    roi_c, roi_l = roi_cls_loss.item(), roi_loc_loss.item()\n",
    "                    self.rpn_cls_losses.append(rpn_c); self.rpn_loc_losses.append(rpn_l)\n",
    "                    self.roi_cls_losses.append(roi_c); self.roi_loc_losses.append(roi_l)\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if epoch % self.TRAIN_config['epoch_freq'] == 0:\n",
    "                mAP = self.val(test_loader)\n",
    "                print('Epoch {} mAP : {:.4f}'.format(epoch+1, 100 * mAP))\n",
    "                if (mAP > self.best_mAP) and (self.TRAIN_config['save']):\n",
    "                    self.best_mAP = mAP\n",
    "                    torch.save(self.model.state_dict(), \n",
    "                               self.TRAIN_config['SAVE_PATH'] + 'epoch_{}.pt'.format(str(epoch+1).zfill(3)))\n",
    "                    print('Saved Best Model')\n",
    "            print()\n",
    "                \n",
    "    def val(self, test_loader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_bboxs, pred_labels, pred_scores = [], [], []\n",
    "            gt_labels, gt_bboxs = [], []\n",
    "            \n",
    "            for images, gt_labels_, gt_bboxs_ in test_loader:\n",
    "                _, _, _, _, pred_labels_, pred_scores_, pred_detections_ = self.model(images)\n",
    "                gt_labels.append(gt_labels_.view(-1))\n",
    "                gt_bboxs.append(gt_bboxs_.view(-1, 4))\n",
    "                \n",
    "                pred_labels.append(pred_labels_.view(-1))\n",
    "                pred_scores.append(pred_scores_.view(-1))\n",
    "                pred_bboxs.append(pred_detections_.view(-1, 4))\n",
    "        \n",
    "        self.model.train()\n",
    "        mAP = voc_eval(pred_bboxs, pred_labels, pred_scores, gt_bboxs, gt_labels, \n",
    "                       self.TEST_config['num_classes'], self.TEST_config['iou_thresh'], self.TEST_config['use_07_metric'])\n",
    "        return mAP\n",
    "    \n",
    "    def demo(self, image_dir):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            image = Image.open(image_dir).convert('RGB')\n",
    "            image = transforms.Resize(size=self.DEMO_config['min_size'])(image)\n",
    "            image_tensor = transforms.ToTensor()(image)\n",
    "            image_norm_tensor = transforms.Normalize(mean=self.DEMO_config['mean'], std=self.DEMO_config['std'])(image_tensor)\n",
    "            \n",
    "            self.model.FastRCNN.score_thresh = self.DEMO_config['score_thresh']\n",
    "            _, _, _, _, pred_labels_, pred_scores_, pred_detections_ = self.model(image_norm_tensor[None, :, :, :])\n",
    "            \n",
    "        return (image, pred_labels_[0].cpu().numpy(), pred_scores_[0].cpu().numpy(), pred_detections_[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a7e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "FasterRCNN = FasterRCNN_Model(RPN_config, FastRCNN_config, TRAIN_config, TEST_config, DEMO_config, gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea69c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Started...\n"
     ]
    }
   ],
   "source": [
    "FasterRCNN.train(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b982f16",
   "metadata": {},
   "source": [
    "**第八部分 绘制loss曲线**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制RPN Train Loss曲线\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.rcParams['axes.titlesize'] = 30\n",
    "\n",
    "label_fontsize = 25\n",
    "\n",
    "cls_lossline, = plt.plot(rpn_cls_losses, label='CLS')\n",
    "loc_lossline, = plt.plot(rpn_loc_losses, color='red', label='LOC')\n",
    "plt.legend(handles=[cls_lossline, loc_lossline], fontsize=20)\n",
    "plt.xlabel('Step', fontsize=label_fontsize)\n",
    "plt.ylabel('RPN Train Loss', fontsize=label_fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84de5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制Fast R-CNN Train Loss曲线\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.rcParams['axes.titlesize'] = 30\n",
    "\n",
    "label_fontsize = 25\n",
    "\n",
    "cls_lossline, = plt.plot(roi_cls_losses, label='CLS')\n",
    "loc_lossline, = plt.plot(roi_loc_losses, color='red', label='LOC')\n",
    "plt.legend(handles=[cls_lossline, loc_lossline], fontsize=20)\n",
    "plt.xlabel('Step', fontsize=label_fontsize)\n",
    "plt.ylabel('Fast R-CNN Train Loss', fontsize=label_fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3f43b",
   "metadata": {},
   "source": [
    "**第九部分 测试图片**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = FasterRCNN_Model(RPN_config, FastRCNN_config, TRAIN_config, TEST_config, DEMO_config, gpu_id)\n",
    "# Model.model.load_state_dict(torch.load('best_model_vgg.pth'))\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_LABELS = ('__background__', \n",
    "              'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', \n",
    "              'horse','motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "demo_img = 'test.png'\n",
    "\n",
    "img, pred_labels, pred_scores, pred_detections = demo(demo_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(min(6, pred_labels.shape[0])):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    pred_label, pred_score, pred_detection = pred_labels[i], pred_scores[i], pred_detections[i]\n",
    "    image = plt.imshow(img); image.axes.get_xaxis().set_visible(False); image.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.gca().set_title(VOC_LABELS[int(pred_label)] + ' ' + str(round(100 * pred_score, 2)) + '%')\n",
    "    min_x, min_y, max_x, max_y = pred_detection\n",
    "    plt.gca().add_patch(Rectangle((min_x, min_y), max_x-min_x, max_y-min_y, edgecolor='r', facecolor='none'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
